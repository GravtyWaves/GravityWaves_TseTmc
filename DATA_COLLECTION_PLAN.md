# برنامه جامع جمع‌آوری داده‌های TSE (Tehran Stock Exchange)

## نمای کلی پروژه

پروژه جمع‌آوری داده‌های TSE یک سیستم کامل برای جمع‌آوری، ذخیره و مدیریت داده‌های بازار بورس تهران است که شامل قابلیت‌های زیر می‌باشد:

### قابلیت‌های فعلی
- ✅ جمع‌آوری لیست کامل سهام
- ✅ جمع‌آوری لیست صنایع و گروه‌ها
- ✅ جمع‌آوری لیست شاخص‌ها
- ✅ جمع‌آوری تاریخچه قیمت سهام
- ✅ جمع‌آوری تاریخچه حقیقی-حقوقی
- ✅ پشتیبانی از دیتابیس SQLite و PostgreSQL
- ✅ قابلیت به‌روزرسانی مداوم و زمان‌بندی شده
- ✅ سیستم لاگ‌گیری و مانیتورینگ عملکرد

## استراتژی جمع‌آوری داده‌ها

### ۱. انواع داده‌های قابل جمع‌آوری

#### داده‌های پایه (Core Data)
- **لیست سهام**: اطلاعات پایه شامل نماد، نام شرکت، کد وب، صنعت
- **لیست صنایع**: کد صنعت، نام صنعت، کد NAICS
- **لیست شاخص‌ها**: نام شاخص، کد وب شاخص

#### داده‌های سری زمانی (Time Series Data)
- **تاریخچه قیمت**: قیمت باز، بسته، کمینه، بیشینه، حجم، ارزش معاملات
- **تاریخچه حقیقی-حقوقی**: حجم و ارزش معاملات حقیقی و حقوقی
- **تاریخچه شاخص‌ها**: مقدار شاخص، حجم، درصد تغییر

#### داده‌های تکمیلی (Additional Data)
- **سهامداری عمده**: لیست سهامداران عمده در تاریخ‌های مختلف
- **معاملات داخل روزی**: معاملات لحظه‌ای در طول روز معاملاتی
- **تاریخچه دلار**: نرخ ارز برای تحلیل‌های مالی
- **شاخص صنایع**: تاریخچه شاخص هر صنعت

### ۲. استراتژی‌های جمع‌آوری

#### استراتژی A: جمع‌آوری کامل (Full Collection)
```python
# اجرای یک بار در ابتدای راه‌اندازی
collector = TSEDataCollector()
collector.create_database()
collector.load_initial_data()
results = collector.run_full_update()
```

#### استراتژی B: به‌روزرسانی تدریجی (Incremental Update)
```python
# اجرای روزانه/ساعتی برای داده‌های جدید
collector = TSEDataCollector()
collector.update_price_history(days=1)  # فقط داده‌های یک روز گذشته
collector.update_ri_history(days=1)
```

#### استراتژی C: به‌روزرسانی انتخابی (Selective Update)
```python
# به‌روزرسانی فقط سهام خاص یا صنایع خاص
collector.collect_stocks()  # فقط لیست سهام
collector.collect_sectors()  # فقط صنایع
```

### ۳. زمان‌بندی جمع‌آوری داده‌ها

#### برنامه روزانه (Daily Schedule)
- **ساعت ۹:۰۰ صبح**: بررسی اتصال به API و دیتابیس
- **ساعت ۹:۳۰ صبح**: به‌روزرسانی لیست سهام و صنایع (در صورت تغییر)
- **ساعت ۱۰:۰۰ صبح**: شروع معاملات - جمع‌آوری داده‌های لحظه‌ای
- **ساعت ۱۲:۳۰ بعدازظهر**: پایان معاملات - جمع‌آوری داده‌های نهایی روز
- **ساعت ۱۳:۰۰ بعدازظهر**: پردازش و ذخیره داده‌های جمع‌آوری شده

#### برنامه هفتگی (Weekly Schedule)
- **شنبه**: به‌روزرسانی کامل لیست‌ها و داده‌های پایه
- **یکشنبه تا پنج‌شنبه**: به‌روزرسانی روزانه داده‌های معاملاتی

#### برنامه ماهانه (Monthly Schedule)
- **اول هر ماه**: آرشیو داده‌های قدیمی
- **پایان هر ماه**: گزارش عملکرد و تحلیل آماری

### ۴. مدیریت خطا و بازیابی

#### مکانیزم‌های Retry
```python
# پیاده‌سازی در TSEAPIClient
def _make_request(self, endpoint, params=None):
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(url, timeout=API_TIMEOUT)
            response.raise_for_status()
            return response
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY * (attempt + 1))  # Exponential backoff
            else:
                logger.error(f"Failed after {MAX_RETRIES} attempts")
                return None
```

#### استراتژی‌های Fallback
- **API اصلی شکست خورد**: استفاده از API جایگزین
- **API جایگزین هم شکست خورد**: استفاده از داده‌های Mock برای تست
- **مشکل اتصال دیتابیس**: ذخیره موقت در فایل و تلاش مجدد

#### مدیریت Circuit Breaker
- اگر API بیش از X بار شکست خورد، به مدت Y دقیقه غیرفعال شود
- مانیتورینگ مداوم وضعیت API و دیتابیس

### ۵. بهینه‌سازی عملکرد

#### بهینه‌سازی API Calls
- **Batch Processing**: جمع‌آوری داده‌ها به صورت دسته‌ای
- **Parallel Processing**: استفاده از Thread/Process Pool برای درخواست‌های همزمان
- **Caching**: ذخیره موقت داده‌های تکراری

#### بهینه‌سازی دیتابیس
```python
# استفاده از Bulk Insert
def add_price_history(self, history_data):
    if not history_data:
        return 0

    session = self.get_session()
    try:
        session.bulk_insert_mappings(PriceHistory, history_data)
        session.commit()
        return len(history_data)
    except Exception as e:
        session.rollback()
        logger.error(f"Bulk insert failed: {e}")
        return 0
    finally:
        session.close()
```

#### بهینه‌سازی حافظه
- **Streaming**: پردازش داده‌ها به صورت Stream به جای Load کامل
- **Pagination**: تقسیم درخواست‌های بزرگ به صفحات کوچک‌تر
- **Garbage Collection**: آزادسازی حافظه پس از پردازش

### ۶. کیفیت داده‌ها (Data Quality)

#### اعتبار‌سنجی داده‌ها
```python
def validate_stock_data(self, stock_data):
    required_fields = ['ticker', 'name', 'web_id']
    for field in required_fields:
        if not stock_data.get(field):
            raise ValueError(f"Missing required field: {field}")

    # بررسی فرمت داده‌ها
    if not isinstance(stock_data.get('web_id'), str):
        raise ValueError("web_id must be string")

    return True
```

#### پاک‌سازی داده‌ها
- حذف رکوردهای تکراری
- تصحیح داده‌های نامعتبر
- پر کردن داده‌های گمشده با مقادیر پیش‌فرض

#### مانیتورینگ کیفیت
- آمار روزانه کیفیت داده‌ها
- گزارش خطاهای اعتبار‌سنجی
- هشدار برای داده‌های خارج از محدوده

### ۷. مانیتورینگ و گزارش‌گیری

#### متریک‌های کلیدی
- تعداد API Calls موفق/ناموفق
- زمان پاسخ API
- تعداد رکوردهای جمع‌آوری شده
- نرخ خطا در ذخیره‌سازی
- استفاده از منابع سیستم (CPU، حافظه، شبکه)

#### گزارش‌های روزانه
- خلاصه عملکرد جمع‌آوری داده‌ها
- آمار کیفیت داده‌ها
- خطاهای رخ داده و اقدامات انجام شده

#### داشبورد مانیتورینگ
- نمایش لحظه‌ای وضعیت سیستم
- نمودارهای عملکرد
- هشدارهای سیستمی

### ۸. برنامه پیاده‌سازی

#### فاز ۱: بهبود زیرساخت (۱ هفته)
- [ ] پیاده‌سازی Circuit Breaker
- [ ] بهبود سیستم Retry و Exponential Backoff
- [ ] اضافه کردن Parallel Processing
- [ ] بهینه‌سازی Bulk Insert

#### فاز ۲: گسترش قابلیت‌ها (۲ هفته)
- [ ] اضافه کردن جمع‌آوری داده‌های تکمیلی
- [ ] پیاده‌سازی سیستم Caching
- [ ] اضافه کردن Data Validation
- [ ] پیاده‌سازی Streaming Processing

#### فاز ۳: مانیتورینگ و اتوماسیون (۱ هفته)
- [ ] پیاده‌سازی Dashboard مانیتورینگ
- [ ] سیستم گزارش‌گیری خودکار
- [ ] بهبود Logging و Alerting
- [ ] اسکریپت‌های Automation برای Deploy

#### فاز ۴: تست و بهینه‌سازی (۱ هفته)
- [ ] تست Load و Performance
- [ ] تست Error Scenarios
- [ ] بهینه‌سازی بر اساس نتایج تست
- [ ] مستندسازی کامل

### ۹. ریسک‌ها و استراتژی‌های کاهش ریسک

#### ریسک‌های فنی
- **تغییر API TSE**: مانیتورینگ مداوم و داشتن Fallback
- **مشکلات شبکه**: Retry و Circuit Breaker
- **مشکلات دیتابیس**: Backup و Recovery Plan

#### ریسک‌های عملیاتی
- **توقف غیرمنتظره**: Alerting و Auto Recovery
- **افزایش حجم داده‌ها**: Scaling و Archive Strategy
- **تغییر نیازمندی‌ها**: Modular Design و Configuration

### ۱۰. معیارهای موفقیت

#### معیارهای عملکرد
- **Uptime**: ۹۹.۹% زمان کاری سیستم
- **Coverage**: پوشش ۱۰۰% سهام فعال بازار
- **Latency**: حداکثر ۵ دقیقه تأخیر در داده‌های لحظه‌ای
- **Accuracy**: دقت ۹۹.۹% در داده‌های جمع‌آوری شده

#### معیارهای کیفیت
- **Completeness**: کامل بودن داده‌ها (بدون Missing Values)
- **Consistency**: سازگاری داده‌ها در طول زمان
- **Timeliness**: به‌روز بودن داده‌ها

## نتیجه‌گیری

این برنامه جامع، استراتژی کاملی برای جمع‌آوری کارآمد، قابل اعتماد و مقیاس‌پذیر داده‌های TSE ارائه می‌دهد. با اجرای این برنامه، سیستم قادر خواهد بود:

- داده‌های کامل و باکیفیت بازار بورس تهران را جمع‌آوری کند
- به صورت خودکار و بدون نظارت مداوم عمل کند
- در برابر خطاها و مشکلات مقاومت نشان دهد
- عملکرد بهینه و استفاده کارآمد از منابع داشته باشد

برای شروع پیاده‌سازی، پیشنهاد می‌شود با فاز ۱ (بهبود زیرساخت) آغاز شود.
